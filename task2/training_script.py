# -*- coding: utf-8 -*-
"""training_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fwFS5Au0Bg-IVxFCupnE4gh5ZhHO0xMh
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q accelerate datasets evaluate numpy transformers torch sentencepiece wandb pytorch

"""## Imports"""

from datasets import load_dataset, Dataset, DatasetDict
import evaluate
import numpy as np
import random
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    T5Tokenizer,
    T5ForConditionalGeneration
)
from typing import Any, Dict
import wandb

"""## Load Dataset"""

# Global constants
SEED = 42 # Random seed for reproducability
random.seed(SEED)

dataset = load_dataset("allenai/qasper")

"""## Preprocessing Step 1 - Extraction of relevant texts"""

def get_formatted_dataset_from_split(dataset_split):
    abstracts = []
    questions = []
    answers = []

    # Iterate over all articles
    for article in dataset_split:
        qa = article['qas']

        # Iterate over all questions and answers
        for question, answer in zip(qa['question'], qa['answers']):
            unanswerable = False
            # Generate all answer candidates, from which we then randomly sample
            answer_candidates = []
            for question_answer in answer['answer']:
                # Additional check to skip unanswerable questions
                if question_answer['unanswerable']:
                    unanswerable = True

                answer = question_answer['free_form_answer'] if question_answer['free_form_answer'] else ' '.join(question_answer['extractive_spans'])
                answer_candidates.append(answer)

            # If a question is unanswerable, skip it as to not pollute the training data set
            if unanswerable or len(answer_candidates) == 0: continue

            # Finally add relevant objects to training data
            abstracts.append(article['abstract'])
            questions.append(question)
            answers.append(random.choice(answer_candidates)) # Randomly sample from available answers

    # DEBUG: Sanity check
    assert len(abstracts) == len(questions) == len(answers)

    return Dataset.from_dict({
        'abstract': abstracts,
        'question': questions,
        'answer': answers
    }).with_format("torch")

# Task 4 - Initial Preprocessing of qasper dataset
train_dataset = get_formatted_dataset_from_split(dataset['train'])
test_dataset = get_formatted_dataset_from_split(dataset['test'])

print(train_dataset)
print(test_dataset)

# Task 5 - Get validation set from train
train_val_dataset = train_dataset.train_test_split(test_size=0.1, seed=SEED)

dataset = DatasetDict({
    'train': train_val_dataset["train"],
    'test': test_dataset,
    'val': train_val_dataset['test']
})

dataset

"""## Preprocessing Part 2 - Tokenization"""

# Task 6 - Preprocessing Function
tokenizer = T5Tokenizer.from_pretrained("google/t5-efficient-tiny")

def preprocess_function(sample: Dict[str, Any]):
    combined_qa = f"question: {{{sample['question']}}} context: {{{sample['abstract']}}}"
    return {'feature': tokenizer(
                            combined_qa,
                            truncation=True,
                            padding="max_length",
                            max_length=128
                        ),
            'labels':   tokenizer(
                            sample['answer'],
                            truncation=True,
                            padding="max_length",
                            max_length=64).input_ids
            }

# Task 7 - Apply preprocessing function using map
encoded_ds = dataset.map(
                preprocess_function,
                remove_columns=['abstract', 'question', 'answer']
             )

encoded_ds = encoded_ds.flatten() # Flatten such that input_ids and input mask come to the first level

# Rename colummns such that trainer knows what to do
encoded_ds = encoded_ds.rename_columns({
    'feature.attention_mask': 'attention_mask',
    'feature.input_ids':      'input_ids'
})
# encoded_ds = encoded_ds.remove_columns(['attention_mask'])

"""## Load Model"""

# Task 8 - load google/t5-efficient-tiny model with pre-trained weights
model = T5ForConditionalGeneration.from_pretrained("google/t5-efficient-tiny")

"""## Define TrainingArguments"""

# Task 9 - Define Seq2SeqTrainerArguments with learning rate scheduling and weight decay
num_epochs = 5
batch_size = 32
lr = 3e-4
weight_decay = 0.01
gradient_accumulation_steps = 1
lr_scheduler_type = "linear" # ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau']

# Define Seq2SeqTrainerArguments
training_args = Seq2SeqTrainingArguments(
    output_dir = './checkpoints/',
    do_train=True,
    do_eval=True,

    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    num_train_epochs=num_epochs,
    learning_rate=lr,
    lr_scheduler_type=lr_scheduler_type,
    weight_decay=weight_decay,

    save_strategy="epoch",
    logging_strategy='epoch',
    evaluation_strategy="epoch",
    load_best_model_at_end=True,

    report_to="wandb"
)

"""## Evaluation Metric - BLEU"""

# TODO: Calculate BLEU Score as evaluation metric
bleu = evaluate.load('bleu')

def compute_metrics(eval_pred):
    batch_size = 1
    predictions, labels = eval_pred
    num_samples = len(predictions)
    num_batches = (num_samples + batch_size - 1) // batch_size  # Round up division

    total_bleu_score = 0.0

    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = (i + 1) * batch_size

        batch_predictions = predictions[start_idx:end_idx]
        batch_labels = labels[start_idx:end_idx]

        predictions_tokens = [tokenizer.decode(pred) for pred in batch_predictions]
        references_tokens = [tokenizer.decode(ref) for ref in batch_labels]

        batch_bleu_score = bleu.compute(predictions=predictions_tokens, references=references_tokens)
        total_bleu_score += batch_bleu_score

    average_bleu_score = total_bleu_score / num_batches
    return average_bleu_score

"""## Define Trainer"""

# Task 10 - Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=encoded_ds['train'],
    eval_dataset=encoded_ds['test'],
    #compute_metrics=compute_metrics,
    #callbacks=[WandbCallback(log_preds=False)]
)

test = dataset['test'][0]

"""## Train Model"""

# TODO: Register experiment at Weights & Biases  https://wandb.ai/site
wandb_project_name = "nlp-assignment5-task2"
run = wandb.init(project=wandb_project_name,
                     config={"epochs": 5,
                })

trainer.train()

wandb.finish()

"""## Inference"""

# Task 11
#inputs_2 = tokenizer.encode(test['abstract'], test['question'])
#inputs = [tokenizer.encode(test['abstract'], test['question'], return_tensors="pt")]

input_ids_list = []
inputs = tokenizer.encode(test['abstract'], test['question'], return_tensors="pt").to(model.device)
input_ids_list.append(inputs)

# Generate the outputs using the model

output_ids_list = [model.generate(inputs) for inputs in input_ids_list]

# Decode the output tokens back to text for each prompt
output_texts_list = [tokenizer.decode(ids[0]) for ids in output_ids_list]

# Print the generated answers for each prompt
for i, output_text in enumerate(output_texts_list):
    print(f"Generated Answer {i+1}: {output_text}")

first_value = output_ids_list[0][0, 0].item()
second_value = output_ids_list[0][0, 1].item()
print(tokenizer.decode(first_value))  # Output: 0
print(tokenizer.decode(second_value))  # Output: 1

output_ids_list