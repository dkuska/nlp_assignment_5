{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets evaluate accelerate scikit-learn torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    load_dataset, \n",
    "    DatasetDict, \n",
    ")\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - Load the train and test splits from ag_news. Randomly select 10% of the training set as validation.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset = dataset.shuffle(SEED)\n",
    "\n",
    "# dataset[\"train\"] = load_dataset(\"ag_news\", split=\"train[:4000]\") # Note: This is useful for sanity checking the training process. Comment out/Uncomment as necessary\n",
    "\n",
    "train_val_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)  # Split training set into training and validation set\n",
    "\n",
    "# Construct new dataset object from old test, new train and new validation sets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_val_dataset[\"train\"],\n",
    "    'test': dataset[\"test\"],\n",
    "    'val': train_val_dataset['test']\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "# TODO: Check if EOS Token is correctly inserted\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(sample: Dict[str, Any], seq_len: int):\n",
    "    return tokenizer(sample[\"text\"], truncation=True, padding=\"max_length\", max_length=seq_len)\n",
    "\n",
    "encoded_ds = dataset.map(preprocess_function, \n",
    "                         fn_kwargs={\"seq_len\": 256},\n",
    "                         remove_columns=['label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm_probability=0.10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7\n",
    "# TODO: Learning Rate Scheduler, Weight Decay\n",
    "\n",
    "# ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau']\n",
    "lr_scheduler_type = \"linear\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir = './checkpoints/',\n",
    "                                  do_train=True,\n",
    "                                  do_eval=True,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  num_train_epochs=5,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  lr_scheduler_type=lr_scheduler_type,\n",
    "                                  weight_decay=0.1,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset = encoded_ds[\"train\"],\n",
    "                  eval_dataset = encoded_ds[\"test\"],\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9\n",
    "\n",
    "# TODO: Hyper parameter tuning\n",
    "#   - batch size\n",
    "#   - number of epochs\n",
    "#   - weight decay\n",
    "#   - learning rate\n",
    "# Note: Should be executed in Google Colab\n",
    "# Note: Does not yet work as intended... Training loss does not seem to go down...\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Validation and Test Splits with Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10\n",
    "# TODO: Calculate perplexity on validation and test splits\n",
    "# Note: Check out this: https://huggingface.co/docs/transformers/perplexity\n",
    "\n",
    "# from evaluate import load\n",
    "# perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# predictions_train = trainer(encoded_ds[\"test\"])\n",
    "# predictions_val = trainer(encoded_ds[\"val\"])\n",
    "\n",
    "# results_train = perplexity.compute(predictions=predictions_train)\n",
    "# results_val = perplexity.compute(predictions=predictions_val)\n",
    "\n",
    "# print(results_train)\n",
    "# print(results_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11\n",
    "\n",
    "text = \"E-mail scam targets police chief Wiltshire Police warns about <mask> after its fraud squad chief was targeted.\"\n",
    "\n",
    "mask_filler = pipeline('mask-filler', trainer)\n",
    "mask_filler(text, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
