# -*- coding: utf-8 -*-
"""training_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hdM-Ka14JeRm7IBNo8lsEAQE650zrcWB
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q transformers datasets evaluate accelerate scikit-learn torch

from google.colab import drive
### mount your google drive
drive.mount('/content/drive')
model_save_path = "/content/drive/MyDrive/Colab Notebooks/" # change this to your local project folder

"""## Imports"""

from datasets import (
    load_dataset,
    DatasetDict,
)
import torch
from typing import Dict, Any
from transformers import (
    AutoModelForMaskedLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    pipeline
)

model_checkpoint = "distilroberta-base"

"""## Load Dataset"""

# Task 1 - Load the train and test splits from ag_news. Randomly select 10% of the training set as validation.

SEED = 42

dataset = load_dataset("ag_news")
dataset = dataset.shuffle(SEED)

# dataset["train"] = load_dataset("ag_news", split="train[:4000]") # Note: This is useful for sanity checking the training process. Comment out/Uncomment as necessary

train_val_dataset = dataset["train"].train_test_split(test_size=0.1, seed=SEED)  # Split training set into training and validation set

# Construct new dataset object from old test, new train and new validation sets
dataset = DatasetDict({
    'train': train_val_dataset["train"],
    'test': dataset["test"],
    'val': train_val_dataset['test']
})

"""## Preprocessing Function"""

# Task 3
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(example, seq_len: int):
    result = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=seq_len,
        return_special_tokens_mask=True,
    )
    result['labels'] = result['input_ids'].copy() # Copy input_ids for later use during training
    return result

encoded_ds = dataset.map(
    preprocess_function,
    batched=True,
    fn_kwargs={"seq_len": 256},
    remove_columns=['text', 'label']
)

print(encoded_ds)

"""## Data Collator"""

# Task 4
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,
                                                mlm_probability=0.10)

"""## Load Model"""

# Task 5
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)

# Task 6
print(model)

"""## Define TrainingArguments"""

# Task 7

num_epochs = 12
batch_size = 32
lr = 3e-4
weight_decay = 0.001

# ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau']
lr_scheduler_type = "cosine"

training_args = TrainingArguments(
    output_dir = model_save_path,
    do_train=True,
    do_eval=True,

    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    learning_rate=lr,
    lr_scheduler_type=lr_scheduler_type,
    weight_decay=weight_decay,

    save_strategy="epoch",
    logging_strategy='epoch',
    evaluation_strategy="epoch",
    load_best_model_at_end=True,
)

"""## Define Trainer"""

# Task 8
trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset = encoded_ds["train"],
                  eval_dataset = encoded_ds["val"],
                  data_collator=data_collator)

"""## Train Model"""

# Task 9
# Note: Should be executed in Google Colab

trainer.train()

trainer.save_model(model_save_path + f'/{num_epochs}epochs_{batch_size}batchsize/')

"""## Evaluation on Validation and Test Splits with Perplexity"""

# Task 10
# TODO: Calculate perplexity on validation and test splits
# Note: Check out this: https://huggingface.co/docs/transformers/perplexity

# from evaluate import load
# perplexity = load("perplexity", module_type="metric")

# predictions_train = trainer(encoded_ds["test"])
# predictions_val = trainer(encoded_ds["val"])

# results_train = perplexity.compute(predictions=predictions_train)
# results_val = perplexity.compute(predictions=predictions_val)

# print(results_train)
# print(results_val)

"""## Inference"""

# # Task 11

# text = "E-mail scam targets police chief Wiltshire Police warns about <mask> after its fraud squad chief was targeted."

# mask_filler = pipeline('mask-filler', trainer)
# mask_filler(text, top_k=5)